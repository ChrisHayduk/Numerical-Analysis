{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis - Homework 3\n",
    "## Chris Hayduk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0 , x_n:  3 , x_{n+1}:  2 , Error:  1\n",
      "\n",
      " Iteration:  1 , x_n:  2 , x_{n+1}:  0.8706020782571979 , Error:  1.129397921742802\n",
      "\n",
      " Iteration:  2 , x_n:  0.8706020782571979 , x_{n+1}:  1.198554234317791 , Error:  0.32795215606059314\n",
      "\n",
      " Iteration:  3 , x_n:  1.198554234317791 , x_{n+1}:  1.3229955468185879 , Error:  0.12444131250079682\n",
      "\n",
      " Iteration:  4 , x_n:  1.3229955468185879 , x_{n+1}:  1.2914674589747202 , Error:  0.031528087843867736\n",
      "\n",
      " Iteration:  5 , x_n:  1.2914674589747202 , x_{n+1}:  1.2926813363329115 , Error:  0.0012138773581913398\n",
      "\n",
      " Iteration:  6 , x_n:  1.2926813363329115 , x_{n+1}:  1.2926957264424896 , Error:  1.4390109578155119e-05\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def secant(x0, x1, f, max_n):\n",
    "    fx0 = f(x0)\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    error = abs(x0-x1)\n",
    "    ep = 10**(-10)\n",
    "    \n",
    "    while error > ep and n <= max_n:\n",
    "        print(\"\\n Iteration: \", n, \", x_n: \", x0, \", x_{n+1}: \", x1, \", Error: \", error)\n",
    "        n += 1\n",
    "        fx1 = f(x1)\n",
    "        \n",
    "        if fx1 - fx0 == 0:\n",
    "            print(\"f(x1) = f(x0)); Division by zero\")\n",
    "            break\n",
    "        \n",
    "        x2 = x1 - fx1*(x1-x0)/(fx1-fx0)\n",
    "        \n",
    "        error = abs(x2 - x1)\n",
    "        \n",
    "        x0 = x1\n",
    "        x1 = x2\n",
    "        fx0 = fx1\n",
    "    \n",
    "secant(3, 2, lambda x: math.exp(-x) - math.cos(x), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $g(\\alpha) = \\alpha$ and $|g'(\\alpha)| < 1$. Since $g'$ is continuous, there exists a neighborhood around $\\alpha$ such that $a = \\alpha - \\varepsilon < \\alpha < b = \\alpha + \\varepsilon$ for some $\\varepsilon > 0$ satisfying $|g'(a)|, |g'(\\alpha)|, |g'(b)| < 1$.\n",
    "\n",
    "Since $|g'(x) < 1|$ for all $a \\leq x \\leq b$, any change in $g(x)$ will be smaller than the corresponding change in $x$. Thus, $a \\leq x \\leq b \\implies g(a) \\leq g(x) \\leq g(b)$\n",
    "\n",
    "Furthermore, since $g$ and $g'$ are continuous over $\\mathbb{R}$, we know that $g$ and $g'$ are continuous on $a < \\alpha < b$ as defined above.\n",
    "\n",
    "Thus, we may apply Theorem 3.4.2. \n",
    "\n",
    "By S2 in Theorem 3.4.2, for any initial estimate $x_0$ in $[a, b]$, the iterates $x_n$ will converge to $\\alpha$.\n",
    "\n",
    "Hence, if $x_0$ is sufficiently close to $\\alpha$, then the iteration $x_{n+1} = g(x_n)$ converges to $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Let $g(x) = cosh(x) - 1 - \\frac{1}{2}x^2$. \n",
    "   \n",
    "   $g(0) = cosh(0) - 1 - \\frac{1}{2}0^2 = 1 - 1 - 0 = 0$\n",
    "   \n",
    "   $g'(x) = sinh(x) - x \\implies g'(0) = 0$\n",
    "   \n",
    "   $g''(x) = cosh(x) - 1 \\implies g''(0) = 0$\n",
    "   \n",
    "   $g'''(x) = sinh(x) \\implies g'''(0) = 0$\n",
    "   \n",
    "   $g''''(x) = cosh(x) \\implies g''''(0) = 1 \\neq 0$\n",
    "   \n",
    "   Thus, by 3.60 in the textbook, 0 is a root of multiplicity 4.\n",
    "   \n",
    "b) Since $\\alpha$ is a root of multiplicity $m$ for $f(x)$, we can write $f(x) = (x - \\alpha)^mh(x)$ where $h(x)$ is some continuous function with $h(\\alpha) \\neq 0$.\n",
    "\n",
    "The definition of Big-O notation states that $f(x) = O(g(x))$ as $x \\to a$ if $\\exists c \\in \\mathbb{R}$ such that $|f(x)| \\leq c|g(x)|$ as $x \\to a$.\n",
    "\n",
    "Since $h(x)$ in the equation above is a continuous function, then $h(x)$ attains a maximum on any closed interval $[a,b] \\in \\mathbb{R}$. Let $c = \\max_{[a,b]}|h(x)|$\n",
    "\n",
    "Then, $|f(x)| = |(x - \\alpha)^mh(x)| = |(x - \\alpha)^m||h(x)| \\leq |(x - \\alpha)^m|c$.\n",
    "\n",
    "Now let $g(x) = (x - \\alpha)^m.\n",
    "\n",
    "This yields $|f(x)| = c|g(x)|$, which satisfies the definition of Big-O notation.\n",
    "\n",
    "Thus, $f(x) = O((x - \\alpha)^m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_c(x) = x + cf(x) \\implies g_c'(x) = 1 + cf'(x)$\n",
    "\n",
    "$\\alpha - x_{n+1}$ is an approximation of the error at each step. \n",
    "\n",
    "$\\alpha - x_{n+1} \\approx g_c'(\\alpha)(\\alpha - x_n) \\implies |c| < 1$ will lead to a smaller error term at each iteration and thus faster convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0 , xn:  1 , Error:  1\n",
      "\n",
      " Iteration:  1 , xn:  0.9371428571428569 , Error:  0.06285714285714306\n",
      "\n",
      " Iteration:  2 , xn:  0.8933040293040273 , Error:  0.04383882783882964\n",
      "\n",
      " Iteration:  3 , xn:  0.8631001953922838 , Error:  0.030203833911743527\n",
      "\n",
      " Iteration:  4 , xn:  0.8424870997854577 , Error:  0.020613095606826115\n",
      "\n",
      " Iteration:  5 , xn:  0.8285184471959752 , Error:  0.013968652589482433\n",
      "\n",
      " Iteration:  6 , xn:  0.8191005672521435 , Error:  0.009417879943831697\n",
      "\n",
      " Iteration:  7 , xn:  0.8127736132321934 , Error:  0.006326954019950093\n",
      "\n",
      " Iteration:  8 , xn:  0.8085336806711613 , Error:  0.004239932561032167\n",
      "\n",
      " Iteration:  9 , xn:  0.8056971548348056 , Error:  0.00283652583635563\n",
      "\n",
      " Iteration:  10 , xn:  0.8038016925795347 , Error:  0.00189546225527093\n"
     ]
    }
   ],
   "source": [
    "#x: initial guess\n",
    "#f: function\n",
    "#df: derivative of function\n",
    "#max_n: max number of iterations\n",
    "def newton(x, f, df, max_n):\n",
    "    x0 = x\n",
    "    \n",
    "    n = 0\n",
    "\n",
    "    ep = 10**(-10)\n",
    "\n",
    "    error = 1\n",
    "\n",
    "    while error > ep and n <= max_n:\n",
    "        print(\"\\n Iteration: \", n, \", xn: \", x0, \", Error: \", error)\n",
    "        fx0 = f(x0)\n",
    "        dfx0 = df(x0)\n",
    "\n",
    "        if(dfx0 == 0):\n",
    "            print(\"The derivative is 0.\")\n",
    "            break\n",
    "\n",
    "        x1 = x0 - fx0/dfx0\n",
    "\n",
    "        error = abs(x1 - x0)\n",
    "\n",
    "        x0 = x1\n",
    "\n",
    "        n += 1\n",
    "\n",
    "newton(1, lambda x: x**5 - 2.4*x**3 + 0.64*x**2 + 1.536*x - 0.73728, \n",
    "       lambda x: 5*x**4 - 7.2*x**2 + 1.28*x + 1.536, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Ch 3.5, $\\lambda = \\frac{m-1}{m}$ and $\\alpha - x_n \\approx \\lambda(\\alpha-x_{n-1})$\n",
    "\n",
    "Combining these two results, we get $\\lambda \\approx \\lambda_n \\equiv \\frac{x_n - x_{n-1}}{x_{n-1}-x_{n-2}}$\n",
    "\n",
    "Using Iteration 10:\n",
    "\n",
    "$\\lambda \\approx \\lambda_{10}\\equiv \\frac{x_{10} - x_9}{x_9 - x_8} \\approx 0.668233735 \\approx \\frac{2}{3}$. \n",
    "\n",
    "Thus, we can take the second derivative of $f(x)$ and still preserve $\\alpha$ as a root.\n",
    "\n",
    "$f''(x) = 20x^3 - 14.4x + 1.28$\n",
    "\n",
    "In the case of $f''(x)$, $\\alpha$ is a simple root.\n",
    "\n",
    "In order to run Newton's Method on $f''(x)$, we must also find $f'''(x)$:\n",
    "\n",
    "$f'''(x) = 60x^2 - 14.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration:  0 , xn:  1 , Error:  1\n",
      "\n",
      " Iteration:  1 , xn:  0.8491228070175438 , Error:  0.15087719298245617\n",
      "\n",
      " Iteration:  2 , xn:  0.80417759696571 , Error:  0.04494521005183383\n",
      "\n",
      " Iteration:  3 , xn:  0.800034448988395 , Error:  0.004143147977314965\n",
      "\n",
      " Iteration:  4 , xn:  0.8000000023732067 , Error:  3.4446615188366e-05\n",
      "\n",
      " Iteration:  5 , xn:  0.8 , Error:  2.373206631212099e-09\n"
     ]
    }
   ],
   "source": [
    "newton(1, lambda x: 20*x**3 - 14.4*x + 1.28, \n",
    "       lambda x: 60*x**2 - 14.4, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
